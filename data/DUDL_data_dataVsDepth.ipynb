{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DUDL_data_dataVsDepth.ipynb","provenance":[{"file_id":"1Yp9bgltmsXuxkNPmbKEC1kn7bkBQD5WD","timestamp":1618250296221},{"file_id":"10_geQnah5AvMsm8VDAQwNPhypOXradar","timestamp":1617634658608},{"file_id":"1FtQ99beHYcDFDywLdaPgFm-KjBeI8PvD","timestamp":1615877547147}],"collapsed_sections":[],"authorship_tag":"ABX9TyPi+7fIeCkuNqCReiYH40Iw"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"bhWV8oes-wKR"},"source":["# COURSE: A deep understanding of deep learning\n","## SECTION: More on data\n","### LECTURE: Sample size and network depth\n","#### TEACHER: Mike X Cohen, sincxpress.com\n","##### COURSE URL: udemy.com/course/dudl/?couponCode=202201"]},{"cell_type":"code","metadata":{"id":"j7-LiwqUMGYL"},"source":["# import libraries\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader,TensorDataset\n","from sklearn.model_selection import train_test_split\n","\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","from IPython import display\n","display.set_matplotlib_formats('svg')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j-SP8NPsMNRL"},"source":["# a function that creates data\n","\n","def createSomeData(nPerClust):\n","\n","  A = [ 1, 1 ]\n","  B = [ 5, 1 ]\n","  C = [ 4, 4 ]\n","\n","  # generate data\n","  a = [ A[0]+np.random.randn(nPerClust) , A[1]+np.random.randn(nPerClust) ]\n","  b = [ B[0]+np.random.randn(nPerClust) , B[1]+np.random.randn(nPerClust) ]\n","  c = [ C[0]+np.random.randn(nPerClust) , C[1]+np.random.randn(nPerClust) ]\n","\n","  # true labels\n","  labels_np = np.hstack(( np.zeros((nPerClust)),\n","                          np.ones( (nPerClust)),\n","                        1+np.ones( (nPerClust))  ))\n","\n","  # concatanate into a matrix, then convert to a pytorch tensor\n","  data_np = np.hstack((a,b,c)).T\n","\n","  # NEW: put all outputs into a dictionary\n","  output = {}\n","  output['data'] = torch.tensor(data_np).float()\n","  output['labels'] = torch.tensor(labels_np).long() # note: \"long\" format (integers) for labels\n","\n","  # use scikitlearn to split the data\n","  train_data,test_data, train_labels,test_labels = train_test_split(output['data'], output['labels'], train_size=.9)\n","\n","  # then convert them into PyTorch Datasets (note: already converted to tensors)\n","  train_data = TensorDataset(train_data,train_labels)\n","  test_data  = TensorDataset(test_data,test_labels)\n","\n","  # finally, translate into dataloader objects\n","  batchsize  = 8\n","  output['train_data'] = DataLoader(train_data,batch_size=batchsize,shuffle=True,drop_last=True)\n","  output['test_data']  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n","\n","  return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CMCgk85rPhB2"},"source":["# Check that the function works\n","\n","theData = createSomeData(50)\n","\n","data = theData['data']\n","labels = theData['labels']\n","\n","# show the data\n","fig = plt.figure(figsize=(5,5))\n","plt.plot(data[np.where(labels==0)[0],0],data[np.where(labels==0)[0],1],'bs',alpha=.5)\n","plt.plot(data[np.where(labels==1)[0],0],data[np.where(labels==1)[0],1],'ko',alpha=.5)\n","plt.plot(data[np.where(labels==2)[0],0],data[np.where(labels==2)[0],1],'r^',alpha=.5)\n","plt.title('The qwerties!')\n","plt.xlabel('qwerty dimension 1')\n","plt.ylabel('qwerty dimension 2')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"36x0DIWe3RkB"},"source":["# Create the model"]},{"cell_type":"code","metadata":{"id":"z0YpD6f-j8dG"},"source":["# create a class for the model\n","def createTheQwertyNet(nUnits,nLayers):\n","\n","  class qwertyNet(nn.Module):\n","    def __init__(self,nUnits,nLayers):\n","      super().__init__()\n","\n","      # create dictionary to store the layers\n","      self.layers = nn.ModuleDict()\n","      self.nLayers = nLayers\n","\n","      ### input layer\n","      self.layers['input'] = nn.Linear(2,nUnits)\n","      \n","      ### hidden layers\n","      for i in range(nLayers):\n","        self.layers[f'hidden{i}'] = nn.Linear(nUnits,nUnits)\n","\n","      ### output layer\n","      self.layers['output'] = nn.Linear(nUnits,3)\n","    \n","\n","    # forward pass\n","    def forward(self,x):\n","      # input layer\n","      x = self.layers['input'](x)\n","\n","      # hidden layers\n","      for i in range(self.nLayers):\n","        x = F.relu( self.layers[f'hidden{i}'](x) )\n","      \n","      # return output layer\n","      x = self.layers['output'](x)\n","      return x\n","  \n","  # create the model instance\n","  net = qwertyNet(nUnits,nLayers)\n","  \n","  # loss function\n","  lossfun = nn.CrossEntropyLoss()\n","\n","  # optimizer\n","  optimizer = torch.optim.SGD(net.parameters(),lr=.01)\n","\n","  return net,lossfun,optimizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"krQeh5wYMNla"},"source":["# test the model with fake input\n","\n","nUnitsPerLayer = 12\n","nLayers = 4\n","\n","net,lossf,opt = createTheQwertyNet(nUnitsPerLayer,nLayers)\n","print(net)\n","\n","# input is ten samples\n","input = torch.rand(10,2)\n","net(input)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l4TW9zEu3Ueh"},"source":["# A function that trains the model"]},{"cell_type":"code","metadata":{"id":"7Q0nmoUPmu-5"},"source":["def function2trainTheModel(nUnits,nLayers):\n","\n","  # number of epochs\n","  numepochs = 50\n","  \n","  # create a new model\n","  net,lossfun,optimizer = createTheQwertyNet(nUnits,nLayers)\n","\n","  # initialize losses\n","  losses   = torch.zeros(numepochs)\n","  trainAcc = []\n","  testAcc  = []\n","\n","  # loop over epochs\n","  for epochi in range(numepochs):\n","\n","    # loop over training data batches\n","    batchAcc  = []\n","    batchLoss = []\n","    for X,y in train_data:\n","\n","      # forward pass and loss\n","      yHat = net(X)\n","      loss = lossfun(yHat,y)\n","\n","      # backprop\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","\n","      # loss from this batch\n","      batchLoss.append(loss.item())\n","\n","      # compute accuracy\n","      matches = torch.argmax(yHat,axis=1) == y     # booleans (false/true)\n","      matchesNumeric = matches.float()             # convert to numbers (0/1)\n","      accuracyPct = 100*torch.mean(matchesNumeric) # average and x100 \n","      batchAcc.append( accuracyPct )               # add to list of accuracies\n","    # end of batch loop...\n","\n","    # now that we've trained through the batches, get their average training accuracy\n","    trainAcc.append( np.mean(batchAcc) )\n","\n","    # and get average losses across the batches\n","    losses[epochi] = np.mean(batchLoss)\n","\n","    # test accuracy\n","    X,y = next(iter(test_data)) # extract X,y from test dataloader\n","    with torch.no_grad(): # deactivates autograd\n","      yHat = net(X)\n","      \n","    # compare the following really long line of code to the training accuracy lines\n","    testAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) ) \n","  # end epochs\n","\n","  # function output\n","  return trainAcc,testAcc,losses,net\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"of9E8ClxMNsD"},"source":["### Test the model once with a bit of data, just to make sure the code works.\n","\n","# generate the data\n","theData = createSomeData(200)\n","train_data = theData['train_data']\n","test_data  = theData['test_data']\n","\n","# run the model\n","trainAcc,testAcc,losses,net = function2trainTheModel(80,1)\n","\n","\n","\n","# show the results!\n","fig,ax = plt.subplots(1,2,figsize=(13,4))\n","\n","ax[0].plot(losses.detach())\n","ax[0].set_ylabel('Loss')\n","ax[0].set_xlabel('epoch')\n","ax[0].set_title('Losses')\n","\n","ax[1].plot(trainAcc,label='Train')\n","ax[1].plot(testAcc,label='Test')\n","ax[1].set_ylabel('Accuracy (%)')\n","ax[1].set_xlabel('Epoch')\n","ax[1].set_title('Accuracy')\n","ax[1].legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zmX6K49WMNuy"},"source":["# Now for the experiment!"]},{"cell_type":"code","metadata":{"id":"196UnpfLYldk"},"source":["# before the experiment, configure and confirm the metaparameters\n","\n","# specify the parameters for the model\n","nNodesInModel = 80\n","layersRange   = [ 1,5,10,20 ]\n","nDatapoints   = np.arange(50,551,50)\n","\n","# create a legend for later plotting\n","legend = []\n","\n","# print out the model architectures\n","for lidx,layers in enumerate(layersRange):\n","\n","  # create a model\n","  unitsperlayer = int(nNodesInModel/layersRange[lidx])\n","  net = createTheQwertyNet(unitsperlayer,layers)[0]\n","\n","  # count its parameters (see lecture ANNs:Depth vs. breadth)\n","  nparams = np.sum([ p.numel() for p in net.parameters() if p.requires_grad ])\n","\n","  legend.append( '%s layers, %s units, %s params' %(layers,unitsperlayer,nparams) )\n","  print('This model will have %s layers, each with %s units, totalling %s parameters' %(layers,unitsperlayer,nparams))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2J0jeXG3SnZZ"},"source":["# note: takes ~5 mins\n","\n","# initialize results matrix\n","results = np.zeros((len(nDatapoints),len(layersRange),2))\n","\n","for didx,pnts in enumerate(nDatapoints):\n","  \n","  # create data (note: same data for each layer manipulation!)\n","  theData = createSomeData(pnts)\n","  train_data = theData['train_data']\n","  test_data  = theData['test_data']\n","\n","\n","  # now loop over layers\n","  for lidx,layers in enumerate(layersRange):\n","    \n","    unitsperlayer = int(nNodesInModel/layersRange[lidx])\n","    trainAcc,testAcc,losses,net = function2trainTheModel(unitsperlayer,layers)\n","\n","    # average of last 5 accuracies and losses\n","    results[didx,lidx,0] = np.mean( testAcc[-5:] )\n","    results[didx,lidx,1] = torch.mean(losses[-5:]).item()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hEegaXDsSnca"},"source":["# show the results!\n","\n","fig,ax = plt.subplots(1,2,figsize=(15,5))\n","ax[0].plot(nDatapoints,results[:,:,1],'s-')\n","ax[0].set_ylabel('Loss')\n","ax[0].set_xlabel('Number of data points')\n","ax[0].legend(legend)\n","ax[0].set_title('Losses')\n","\n","ax[1].plot(nDatapoints,results[:,:,0],'o-')\n","ax[1].set_ylabel('Accuracy (%)')\n","ax[1].set_xlabel('Number of data points')\n","ax[1].set_title('Accuracy')\n","ax[1].legend(legend)\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LrzrWK39SniK"},"source":["# Interpretation: \n","#   Learning depends more on the architecture and the nature of the problem, than on the number of parameters."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fExGSRvqvnAl"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FLO0SxB-Snkq"},"source":["# Additional explorations"]},{"cell_type":"code","metadata":{"id":"FOJhMXXISQ0J"},"source":["# 1) The model learns faster and better with the Adam optimizer. In fact, I intentionally used SGD here to make the\n","#    model worse for this demonstration! Change the optimizer to Adam. What do you think is a good learning rate?\n","#    More importantly: Do the conclusions of this experiment hold for the Adam optimizer?\n","# \n","# 2) Add a timer to the experiment loop. Does the training duration relate to the number of layers or the number\n","#    of parameters?\n","# \n","# 3) Do the two deepest models eventually learn if you increase the number of training epochs? (Note: because this \n","#    question is only about the deepest models and because training time will increase, you need only test the two\n","#    models, not all four.)\n","# "],"execution_count":null,"outputs":[]}]}